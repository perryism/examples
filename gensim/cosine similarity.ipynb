{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin \u001b[1m\u001b[36mproduct_research\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mcats_n_dogs\u001b[m\u001b[m                        \u001b[1m\u001b[36msentiment_labelled_sentences\u001b[m\u001b[m\r\n",
      "housing.data.txt                   \u001b[1m\u001b[36mstocks\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/workspace/ipython_notebooks/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "google_news_vectors_file = '~/workspace/ipython_notebooks/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# this loads pre-trained word embeddings model - based on Google News corpus - this is a 3.4G file\n",
    "word2vec = KeyedVectors.load_word2vec_format(google_news_vectors_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare(a, b):\n",
    "    to_vec = lambda w: word2vec.word_vec(w).reshape(1,-1)\n",
    "    return cosine_similarity(to_vec(a),to_vec(b)).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65109581"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890863\n",
      "0.715575\n",
      "0.517165\n",
      "0.199475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#outputs the average word2vec for words in this sentence\n",
    "def average_vec(sentence):\n",
    "    words = sentence.split()\n",
    "    word_vecs = [word2vec.word_vec(w) for w in words]\n",
    "    return (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n",
    "\n",
    "compare = lambda a,b: cosine_similarity(average_vec(a),average_vec(b)).sum()\n",
    "\n",
    "print compare('Quick fox jumps over dog','Fast fox jumps over puppy')\n",
    "# 0.89086312 - basically the same\n",
    "\n",
    "print compare('Quick fox jumps over dog','Fast animal jumps over another one')\n",
    "# 0.71557522 - quite similar\n",
    "\n",
    "print compare('Fruit fell from the tree','An apple has fallen')\n",
    "# 0.51716453 - still significant similarity - even though there is not a single shared word\n",
    "\n",
    "print compare('Quick fox jumps over dog','The judge entered the courtroom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.7118192315101624)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['king','woman'],negative=['man'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# first we need to tokenize our text, turning sequence of words into\n",
    "# sequence of numbers\n",
    "\n",
    "vocabulary_size = 10000 # number of supported words, size of our vocabulary\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "\n",
    "texts = ['This is our super simple',\n",
    "         'Corpus of texts we will process',\n",
    "         'IRL you would load this data from somewhere'\n",
    "        ]\n",
    "\n",
    "tokenizer = Tokenizer(vocabulary_size)\n",
    "tokenizer.fit_on_texts(texts) # we fit tokenizer on texts we will process\n",
    "sequences = tokenizer.texts_to_sequences(texts) # here the conversion to tokens happens\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# let's pad these sequences so all have equal size\n",
    "data = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH)\n",
    "\n",
    "# let's use pre-trained word2vec again\n",
    "word2vec = KeyedVectors.load_word2vec_format('../input/GoogleNews-vectors-negative300.bin', \\\n",
    "        binary=True)\n",
    "VECTOR_DIMENSION = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, VECTOR_DIMENSION)) #word to vec - maps word id (from tokenizer) into vector space\n",
    "\n",
    "# fill this matrix with values from pre-trained word2vec\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "        vocabulary_size, # how many words are mapped into vectors\n",
    "        VECTOR_DIMENSION, # size of output vector dimension (we use pre-trained model with vectors of 300 values)\n",
    "        weights=[embedding_matrix], # we initialize weight from pre-trained model\n",
    "        input_length=MAX_SENTENCE_LENGTH, # how many words in the sentence we process\n",
    "        trainable=False) # we will not update this layer\n",
    "\n",
    "lstm_output_size = 30\n",
    "lstm_layer = LSTM(\n",
    "    lstm_output_size) # number of outputs\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='int32') # the input takes \n",
    "embedded_sentence = embedding_layer(sentence_input)\n",
    "lstm_layer = lstm_layer(embedded_sentence)\n",
    "\n",
    "# you add all deep layers here - let's say we have a single one\n",
    "size_of_dense = 10\n",
    "deep_layer = Dense(\n",
    "        size_of_dense,\n",
    "        activation='sigmoid'\n",
    "    )(lstm_layer)\n",
    "\n",
    "# and now let's assume we have output layer for binary classification task:\n",
    "prediction = Dense(1, activation='sigmoid')(deep_layer)\n",
    "\n",
    "model = Model(inputs=[sentence_input], \\\n",
    "        outputs=prediction)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['acc'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
